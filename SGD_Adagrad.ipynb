{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMltphz30gjlDPEGpHwlGc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayalvizhiT513/Gradient_Descent_Comparison/blob/main/SGD_Adagrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adagrad (Adaptive Gradient Algorithm)**\n",
        "Whatever the optimizer we learned till SGD with momentum, the learning rate remains constant. In Adagrad optimizer, there is no momentum concept so, it is much simpler compared to SGD with momentum.\n",
        "\n",
        "The idea behind Adagrad is to use different learning rates for each parameter base on iteration. The reason behind the need for different learning rates is that the learning rate for sparse features parameters needs to be higher compare to the dense features parameter because the frequency of occurrence of sparse features is lower.\n",
        "\n",
        "* Equation:\n",
        "\n",
        "![link text](https://miro.medium.com/v2/resize:fit:828/format:webp/1*XWvo73EMLhIeGs35xkimVw.png)\n",
        "\n",
        "In the above Adagrad optimizer equation, the learning rate has been modified in such a way that it will automatically decrease because the summation of the previous gradient square will always keep on increasing after every time step.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aja8Wkroy5Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# Step 1: Initialize weights and learning rate\n",
        "w_0 = 0.8260560647266798\n",
        "w_1 = 0.5782539087214469\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Step 2: Load data from CSV file\n",
        "data = np.genfromtxt('randXY.csv', delimiter=',', skip_header=1)  # Adjust the filename accordingly\n",
        "\n",
        "# Extract X and Y from the loaded data\n",
        "X = data[:, 0]  # Assuming the first column is X\n",
        "Y = data[:, 1]  # Assuming the second column is Y\n",
        "\n",
        "print(\"Initial w0: \", w_0, \"\\n Initial w1: \", w_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUCy_KaLzYYb",
        "outputId": "1588ccc9-59a9-4b0e-a88c-51fc00f3bf31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial w0:  0.8260560647266798 \n",
            " Initial w1:  0.5782539087214469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def percentage_difference(value1, value2):\n",
        "    return (np.abs(value1 - value2) / ((value1 + value2) / 2)) * 100"
      ],
      "metadata": {
        "id": "USwaIpt40Ac9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot epoch vs. loss\n",
        "def plot_loss_vs_epoch(loss_history, algorithm_name):\n",
        "    plt.plot(range(len(loss_history)), loss_history, label=algorithm_name)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{algorithm_name} - Epoch vs. Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7i3DiLXd0GTx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def update_w_with_adagrad(w, learning_rate, a, gradient_w, e = 1e-8):\n",
        "    # Update learning_rate\n",
        "    learning_rate = learning_rate / sqrt(a + e)\n",
        "    print(learning_rate)\n",
        "    # Update w0\n",
        "    w = w - (learning_rate * gradient_w)\n",
        "\n",
        "    return w, learning_rate"
      ],
      "metadata": {
        "id": "cNdfhfL60Ick"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define SGD function\n",
        "def sgd_one_sample(X, Y, w0, w1, learning_rate, epochs=6000, tol=1, consecutive_instances=10):\n",
        "    n = len(X)\n",
        "    prev_loss = float('inf')\n",
        "    count = 0\n",
        "    a = 0\n",
        "\n",
        "    # Initialize previous gradients and accumulators\n",
        "    learning_rate_w0 = 0.01\n",
        "    learning_rate_w1 = 0.01\n",
        "    a_w0 = 0.0\n",
        "    a_w1 = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(n):\n",
        "            # Select one random data point\n",
        "            random_index = np.random.randint(0, n)\n",
        "            x_i = X[random_index]\n",
        "            y_i = Y[random_index]\n",
        "\n",
        "            # Calculate prediction and loss for the selected point\n",
        "            prediction = w0 + w1 * x_i\n",
        "            loss = (y_i - prediction)**2\n",
        "\n",
        "            # Calculate gradients\n",
        "            gradient_w0 = -2 * (y_i - prediction)\n",
        "            gradient_w1 = -2 * (y_i - prediction) * x_i\n",
        "\n",
        "            # Update weights\n",
        "            w0, learning_rate_w0 = update_w_with_adagrad(w0, learning_rate_w0, a_w0, gradient_w0)\n",
        "            w1, learning_rate_w1 = update_w_with_adagrad(w1, learning_rate_w1, a_w1, gradient_w1)\n",
        "\n",
        "            # Update a for next iteration\n",
        "            a_w0 += gradient_w0**2\n",
        "            a_w1 += gradient_w1**2\n",
        "\n",
        "            if learning_rate_w0 == 0 and learning_rate_w1 == 0:\n",
        "              break\n",
        "\n",
        "\n",
        "        # Calculate overall loss for monitoring\n",
        "        predictions = w0 + w1 * X\n",
        "        overall_loss = np.mean((Y - predictions)**2)\n",
        "\n",
        "        percent_diff = percentage_difference(prev_loss, overall_loss)\n",
        "        if percent_diff < tol:\n",
        "          count += 1\n",
        "        else:\n",
        "          count = 0\n",
        "\n",
        "        # Print loss for monitoring\n",
        "        if epoch % 500 == 0:\n",
        "            #print(f\"learning_rate_w0: {learning_rate_w0}, learning_rate_w1: {learning_rate_w1}\")\n",
        "            print(f\"Epoch {epoch}, Loss: {overall_loss}\")\n",
        "\n",
        "        # the model will stop learning as the learning rate is 0\n",
        "        if learning_rate_w0 == 0 or learning_rate_w1 == 0:\n",
        "            print(f\"learning_rate_w0: {learning_rate_w0}, learning_rate_w1: {learning_rate_w1}\")\n",
        "            print(f\"Epoch {epoch}, Loss: {overall_loss}\")\n",
        "            break\n",
        "\n",
        "        # Update the stopping criteria to consider non-inf values\n",
        "        if count >= consecutive_instances:\n",
        "            print(f\"Epoch {epoch}, Loss: {overall_loss}\")\n",
        "            print(\"Converged! \", count)\n",
        "            break\n",
        "\n",
        "        # Append loss to the history\n",
        "        loss_history_sgd_one_sample.append(overall_loss)\n",
        "\n",
        "        # Update previous loss for the next iteration\n",
        "        prev_loss = overall_loss\n",
        "\n",
        "    return w0, w1\n",
        "\n",
        "# Step 4: Run SGD with one training sample at a time\n",
        "w_0 = 0.8260560647266798\n",
        "w_1 = 0.5782539087214469\n",
        "learning_rate = 0.01\n",
        "loss_history_sgd_one_sample = []\n",
        "w0_sgd_one_sample, w1_sgd_one_sample = sgd_one_sample(X, Y, w_0, w_1, learning_rate)\n",
        "print(f\"Final weights for SGD with one sample: w0={w0_sgd_one_sample}, w1={w1_sgd_one_sample}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp1Rx_cf0WIp",
        "outputId": "39cb2a30-9db0-400a-f6d1-d4ca45be29fa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0\n",
            "100.0\n",
            "8.018772554696074\n",
            "15.536575201001547\n",
            "0.0022840446933006784\n",
            "0.005523322290864387\n",
            "1.892654009386141e-08\n",
            "5.899913182051107e-08\n",
            "1.1181819680776095e-13\n",
            "4.513205753698324e-13\n",
            "6.094769735562881e-19\n",
            "3.4311821889024295e-18\n",
            "3.0274287262834344e-24\n",
            "2.5480211482889948e-23\n",
            "1.4356865868997696e-29\n",
            "1.8896355452198665e-28\n",
            "5.899617538607427e-35\n",
            "1.1361879769379344e-33\n",
            "2.10912605771523e-40\n",
            "5.269730068598672e-39\n",
            "7.263851893891709e-46\n",
            "2.4309036631357446e-44\n",
            "2.296604388333853e-51\n",
            "1.00676883063019e-49\n",
            "7.036539799001914e-57\n",
            "4.145923715989854e-55\n",
            "2.1020928047202692e-62\n",
            "1.7024314554212093e-60\n",
            "6.140950831065224e-68\n",
            "6.976712779307391e-66\n",
            "1.7658581899542143e-73\n",
            "2.8583046179271997e-71\n",
            "4.909330379826512e-79\n",
            "1.1542999680568876e-76\n",
            "1.305860835741732e-84\n",
            "4.4759034612554257e-82\n",
            "3.320344359727879e-90\n",
            "1.6493155011326462e-87\n",
            "8.295256860649703e-96\n",
            "6.0634491015656925e-93\n",
            "2.0342622010840526e-101\n",
            "2.221522652812938e-98\n",
            "4.7420847019841893e-107\n",
            "7.398823775109126e-104\n",
            "1.0883871033225615e-112\n",
            "2.458438780830822e-109\n",
            "2.3856163962296043e-118\n",
            "7.494623101523652e-115\n",
            "5.156483494841577e-124\n",
            "2.2799953847244573e-120\n",
            "1.0965330803902281e-129\n",
            "6.906126675814293e-126\n",
            "2.2832049749190086e-135\n",
            "2.0676039145500705e-131\n",
            "4.677625457122333e-141\n",
            "6.156870716478669e-137\n",
            "9.277375658299534e-147\n",
            "1.74459953280058e-142\n",
            "1.8011765883644768e-152\n",
            "4.863233994932561e-148\n",
            "3.3897817738056684e-158\n",
            "1.2872423431983326e-153\n",
            "6.3395317309159605e-164\n",
            "3.4069343269608724e-159\n",
            "1.1781500088028628e-169\n",
            "9.016328099327369e-165\n",
            "2.1561226457842787e-175\n",
            "2.368155438144562e-170\n",
            "3.852506507129662e-181\n",
            "6.029664240732601e-176\n",
            "6.815129423944926e-187\n",
            "1.532255028865141e-181\n",
            "1.1987400118971414e-192\n",
            "3.893417271341428e-187\n",
            "2.062939063377707e-198\n",
            "9.619863961225921e-193\n",
            "3.5051518590779636e-204\n",
            "2.3629056279113364e-198\n",
            "5.822939070986958e-210\n",
            "5.608101971522519e-204\n",
            "9.448058206532402e-216\n",
            "1.277908070814542e-209\n",
            "1.4960008168418993e-221\n",
            "2.779273489163117e-215\n",
            "2.354102319772608e-227\n",
            "6.041586922598022e-221\n",
            "3.6900557420497113e-233\n",
            "1.3133097911431039e-226\n",
            "5.75361578438103e-239\n",
            "2.8542138146263662e-232\n",
            "8.865364947976156e-245\n",
            "6.161004874452346e-238\n",
            "1.3529515562425598e-250\n",
            "1.3251074701876157e-243\n",
            "2.0455203710582485e-256\n",
            "2.8400910461500367e-249\n",
            "3.0521228391136093e-262\n",
            "6.020440888876478e-255\n",
            "4.5176040915568665e-268\n",
            "1.2732619262434312e-260\n",
            "6.664200307016457e-274\n",
            "2.6928051547741333e-266\n",
            "9.788935866874475e-280\n",
            "5.694356642318967e-272\n",
            "1.4255931453902376e-285\n",
            "1.2002458844086277e-277\n",
            "2.0376208167662312e-291\n",
            "2.443193040902473e-283\n",
            "2.8720869833428612e-297\n",
            "4.893677879623367e-289\n",
            "3.9816732570900284e-303\n",
            "9.539104666434892e-295\n",
            "5.44260815070461e-309\n",
            "1.8260149698832248e-300\n",
            "7.33813561e-315\n",
            "3.4346769888767747e-306\n",
            "9.827e-321\n",
            "6.44786686492e-312\n",
            "0.0\n",
            "1.209998e-317\n",
            "0.0\n",
            "2.5e-323\n",
            "0.0\n",
            "0.0\n",
            "Epoch 0, Loss: 2426129233.0811987\n",
            "learning_rate_w0: 0.0, learning_rate_w1: 0.0\n",
            "Epoch 0, Loss: 2426129233.0811987\n",
            "Final weights for SGD with one sample: w0=-26628.525501858854, w1=-42541.71007768542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d4fdf7de7d20>:2: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return (np.abs(value1 - value2) / ((value1 + value2) / 2)) * 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, despite not having to manually tune the learning rate there is one huge disadvantage i.e due to monotonically decreasing learning rates, at some point in time step, the model will stop learning as the learning rate is almost close to 0."
      ],
      "metadata": {
        "id": "EM66L3XgNsLh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsJ0-6Oy4NBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}